pipeline {
    agent any

    parameters {
        choice(name: 'ENVIRONMENT', choices: ['prod', 'dev'], description: 'Target environment for performance tests (default: prod)')
        choice(name: 'TEST_TYPE', choices: ['MixedWorkloadUser', 'ProductServiceLoadTest', 'OrderServiceStressTest', 'UserAuthenticationLoadTest', 'ECommercePurchaseUser'], description: 'Type of performance test to run')
        string(name: 'USERS', defaultValue: '100', description: 'Number of concurrent users')
        string(name: 'SPAWN_RATE', defaultValue: '10', description: 'Users spawned per second')
        string(name: 'RUN_TIME', defaultValue: '5m', description: 'Test duration (e.g., 5m, 10m, 1h)')
        booleanParam(name: 'HEADLESS', defaultValue: true, description: 'Run in headless mode (no web UI)')
        string(name: 'MINIKUBE_IP', defaultValue: '192.168.49.2', description: 'Minikube container IP address')
        string(name: 'API_GATEWAY_NODEPORT', defaultValue: '32281', description: 'API Gateway NodePort (run: kubectl get svc api-gateway -n prod -o jsonpath=\'{.spec.ports[0].nodePort}\')')
    }

    environment {
        K8S_NAMESPACE = "${params.ENVIRONMENT}"
        REPORT_NAME = "performance-report-${params.ENVIRONMENT}-${BUILD_NUMBER}"
    }

    options {
        buildDiscarder(logRotator(numToKeepStr: '20'))
        timeout(time: 2, unit: 'HOURS')
        timestamps()
    }

    stages {
        stage('Initialize') {
            steps {
                script {
                    echo """
========================================
  Performance Tests
========================================
Environment: ${params.ENVIRONMENT}
Test Type: ${params.TEST_TYPE}
Users: ${params.USERS}
Spawn Rate: ${params.SPAWN_RATE}
Duration: ${params.RUN_TIME}
Headless: ${params.HEADLESS}
========================================
"""
                }
            }
        }

        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Get API Gateway URL') {
            steps {
                script {
                    echo "üåê Getting API Gateway URL..."

                    sh """
                        set -e

                        # Get Minikube IP (try Docker, fallback to parameter)
                        MINIKUBE_IP="${params.MINIKUBE_IP}"
                        if command -v docker &> /dev/null; then
                            DOCKER_IP=\$(docker inspect minikube -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' 2>/dev/null || echo "")
                            if [ ! -z "\$DOCKER_IP" ]; then
                                MINIKUBE_IP="\$DOCKER_IP"
                                echo "‚úì Minikube IP (from Docker): \$MINIKUBE_IP"
                            else
                                echo "‚úì Minikube IP (from parameter): \$MINIKUBE_IP"
                            fi
                        else
                            echo "‚úì Minikube IP (from parameter): \$MINIKUBE_IP"
                        fi

                        # Get NodePort (try kubectl, fallback to parameter)
                        NODE_PORT="${params.API_GATEWAY_NODEPORT}"
                        if command -v kubectl &> /dev/null; then
                            KUBECTL_PORT=\$(kubectl --insecure-skip-tls-verify get svc api-gateway -n ${K8S_NAMESPACE} -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null || echo "")
                            if [ ! -z "\$KUBECTL_PORT" ]; then
                                NODE_PORT="\$KUBECTL_PORT"
                                echo "‚úì NodePort (from kubectl): \$NODE_PORT"
                            else
                                echo "‚úì NodePort (from parameter): \$NODE_PORT"
                            fi
                        else
                            echo "‚úì NodePort (from parameter): \$NODE_PORT"
                        fi

                        # Build and save URL
                        API_URL="http://\$MINIKUBE_IP:\$NODE_PORT"
                        echo "\$API_URL" > /tmp/api-gateway-url.txt
                        echo ""
                        echo "========================================="
                        echo "API Gateway URL: \$API_URL"
                        echo "========================================="
                    """

                    // Read the URL from file and set as environment variable
                    env.API_GATEWAY_URL = sh(script: 'cat /tmp/api-gateway-url.txt', returnStdout: true).trim()
                }
            }
        }

        stage('Verify Services Health') {
            steps {
                script {
                    echo "üè• Checking services health..."
                    sh """
                        set -e

                        echo "Testing connectivity to ${env.API_GATEWAY_URL}"
                        echo ""

                        # Test connectivity with retries
                        for i in 1 2 3 4 5; do
                            HTTP_CODE=\$(curl -s -o /dev/null -w "%{http_code}" ${env.API_GATEWAY_URL}/app/api/products --max-time 5 || echo "000")

                            if [ "\$HTTP_CODE" != "000" ]; then
                                echo "‚úì API Gateway is reachable (HTTP \$HTTP_CODE)"
                                break
                            fi

                            if [ \$i -lt 5 ]; then
                                echo "Attempt \$i/5: Got HTTP \$HTTP_CODE, retrying in 5 seconds..."
                                sleep 5
                            else
                                echo "‚ùå ERROR: Cannot reach API Gateway at ${env.API_GATEWAY_URL} after 5 attempts"
                                echo ""
                                echo "API Gateway pods:"
                                kubectl --insecure-skip-tls-verify get pods -n ${K8S_NAMESPACE} -l app=api-gateway
                                echo ""
                                echo "API Gateway service:"
                                kubectl --insecure-skip-tls-verify get svc api-gateway -n ${K8S_NAMESPACE}
                                exit 1
                            fi
                        done

                        echo ""
                        echo "Kubernetes Services Status:"
                        kubectl --insecure-skip-tls-verify get pods -n ${K8S_NAMESPACE} | grep -E "(api-gateway|product-service|order-service|user-service)" || true
                    """
                }
            }
        }

        stage('Install Dependencies') {
            steps {
                script {
                    echo "üì¶ Installing Locust and dependencies..."
                    sh """
                        cd tests/performance

                        # Clean up and recreate virtual environment
                        echo "Creating fresh Python virtual environment..."
                        rm -rf venv
                        python3 -m venv venv

                        # Activate virtual environment and install dependencies
                        . venv/bin/activate

                        echo "Upgrading pip..."
                        pip install --upgrade pip

                        echo "Installing requirements..."
                        pip install -r requirements.txt

                        echo "Verifying Locust installation..."
                        locust --version

                        deactivate
                    """
                }
            }
        }

        stage('Run Performance Tests') {
            steps {
                script {
                    echo """
========================================
üöÄ Starting Performance Tests
========================================
Target: ${env.API_GATEWAY_URL}
Test Type: ${params.TEST_TYPE}
Users: ${params.USERS}
Spawn Rate: ${params.SPAWN_RATE}
Duration: ${params.RUN_TIME}
========================================
"""

                    def headlessFlag = params.HEADLESS ? '--headless' : ''
                    def reportFlags = params.HEADLESS ? "--html ${env.REPORT_NAME}.html --csv ${env.REPORT_NAME}" : ''

                    try {
                        sh """
                            cd tests/performance

                            # Activate virtual environment
                            . venv/bin/activate

                            locust -f locustfile.py ${params.TEST_TYPE} \
                                --host=${env.API_GATEWAY_URL} \
                                --users ${params.USERS} \
                                --spawn-rate ${params.SPAWN_RATE} \
                                --run-time ${params.RUN_TIME} \
                                ${headlessFlag} \
                                ${reportFlags}

                            deactivate
                        """
                        
                        echo """
========================================
‚úì Performance Tests Completed
========================================
"""
                    } catch (Exception e) {
                        echo "‚ö†Ô∏è Performance tests encountered issues: ${e.message}"
                        echo "This may indicate performance problems or test failures"
                        
                        // Don't fail the pipeline, just mark as unstable
                        currentBuild.result = 'UNSTABLE'
                    }
                }
            }
        }

        stage('Analyze Results') {
            when {
                expression { params.HEADLESS }
            }
            steps {
                script {
                    echo "üìä Analyzing performance test results..."
                    
                    sh """
                        cd tests/performance
                        
                        if [ -f "${env.REPORT_NAME}_stats.csv" ]; then
                            echo ""
                            echo "========================================="
                            echo "  Performance Summary"
                            echo "========================================="
                            head -20 ${env.REPORT_NAME}_stats.csv
                            echo ""
                            
                            # Check for high error rates
                            ERRORS=\$(awk -F, 'NR>1 {sum+=\$8} END {print sum}' ${env.REPORT_NAME}_stats.csv || echo "0")
                            TOTAL=\$(awk -F, 'NR>1 {sum+=\$2} END {print sum}' ${env.REPORT_NAME}_stats.csv || echo "1")
                            ERROR_RATE=\$(echo "scale=2; \$ERRORS / \$TOTAL * 100" | bc || echo "0")
                            
                            echo "Total Requests: \$TOTAL"
                            echo "Failed Requests: \$ERRORS"
                            echo "Error Rate: \$ERROR_RATE%"
                            
                            # Fail if error rate is too high (> 5%)
                            if (( \$(echo "\$ERROR_RATE > 5.0" | bc -l) )); then
                                echo "‚ùå ERROR RATE TOO HIGH: \$ERROR_RATE%"
                                exit 1
                            fi
                        else
                            echo "‚ö†Ô∏è No stats file found"
                        fi
                    """
                }
            }
        }

        stage('Publish Results') {
            when {
                expression { params.HEADLESS && fileExists("tests/performance/${env.REPORT_NAME}.html") }
            }
            steps {
                script {
                    echo "üì§ Publishing performance test reports..."
                    
                    publishHTML([
                        reportDir: 'tests/performance',
                        reportFiles: "${env.REPORT_NAME}.html",
                        reportName: "Performance Test Report - ${params.ENVIRONMENT}",
                        keepAll: true,
                        alwaysLinkToLastBuild: true
                    ])
                    
                    // Archive CSV results
                    archiveArtifacts artifacts: "tests/performance/${env.REPORT_NAME}*.csv", allowEmptyArchive: true
                    archiveArtifacts artifacts: "tests/performance/${env.REPORT_NAME}.html", allowEmptyArchive: true
                    
                    echo "‚úì Reports published successfully"
                }
            }
        }

        stage('SonarQube Analysis - Performance Tests') {
            steps {
                script {
                    echo "üìä Analyzing performance test code with SonarQube..."

                    try {
                        sh """
                            cd tests/performance

                            # Download SonarQube Scanner if not present
                            if [ ! -d "sonar-scanner" ]; then
                                echo "Downloading SonarQube Scanner..."
                                wget -q https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-5.0.1.3006-linux.zip  
                                unzip -q sonar-scanner-cli-5.0.1.3006-linux.zip
                                mv sonar-scanner-5.0.1.3006-linux sonar-scanner
                                rm sonar-scanner-cli-5.0.1.3006-linux.zip
                            fi

                            # Run SonarQube analysis
                            echo "Running SonarQube analysis..."
                            ./sonar-scanner/bin/sonar-scanner \
                                -Dsonar.host.url=http://192.168.49.2:9000 \
                                -Dsonar.token=squ_1037e66e9bc493d2a288dbca5a9cb503f0637c93 \
                                -Dsonar.projectKey=ecommerce-performance-tests \
                                -Dsonar.projectName="E-Commerce Performance Tests" \
                                -Dsonar.projectVersion=1.0 \
                                -Dsonar.sources=. \
                                -Dsonar.inclusions=**/*.py \
                                -Dsonar.exclusions=**/venv/**,**/__pycache__/**,**/sonar-scanner/** \
                                -Dsonar.language=py \
                                -Dsonar.python.version=3.13 \
                                -Dsonar.sourceEncoding=UTF-8

                            echo "‚úì SonarQube analysis completed"
                            echo "View results at: http://192.168.49.2:9000/dashboard?id=ecommerce-performance-tests"
                        """
                    } catch (Exception e) {
                        echo "‚ö†Ô∏è SonarQube analysis failed: ${e.message}"
                        echo "This is non-blocking, continuing pipeline..."
                    }
                }
            }
        }

        stage('Summary') {
            steps {
                script {
                    echo """
========================================
  Performance Test Summary
========================================
‚úì Environment: ${params.ENVIRONMENT}
‚úì Test Type: ${params.TEST_TYPE}
‚úì Users: ${params.USERS}
‚úì Spawn Rate: ${params.SPAWN_RATE}
‚úì Duration: ${params.RUN_TIME}
‚úì Report: ${env.REPORT_NAME}.html

View Results:
- HTML Report: Check Published HTML Reports
- CSV Data: Check Build Artifacts
- Full Logs: See console output above
- SonarQube: http://192.168.49.2:9000/dashboard?id=ecommerce-performance-tests

Next Steps:
1. Review report for performance bottlenecks
2. Check error rates and response times
3. Compare with previous test runs
4. Review SonarQube for code quality issues
5. Scale services if needed
========================================
"""
                }
            }
        }
    }

    post {
        success {
            script {
                echo "‚úÖ Performance tests completed successfully"
            }
        }
        unstable {
            script {
                echo "Check the report for high error rates or slow response times"
            }
        }
        failure {
            script {
                echo "‚ùå Pipeline failed. Check logs for details."
            }
        }
        always {
            script {
                echo "üßπ Cleaning up resources..."

                // Cleanup temporary files
                sh '''
                    rm -f /tmp/api-gateway-url.txt || true
                '''
            }

            cleanWs(deleteDirs: true, patterns: [
                [pattern: 'tests/performance/*.log', type: 'INCLUDE']
            ])
        }
    }
}