pipeline {
    agent any

    parameters {
        choice(name: 'ENVIRONMENT', choices: ['prod', 'dev'], description: 'Target environment for performance tests (default: prod)')
        choice(name: 'TEST_TYPE', choices: ['MixedWorkloadUser', 'ProductServiceLoadTest', 'OrderServiceStressTest', 'UserAuthenticationLoadTest', 'ECommercePurchaseUser'], description: 'Type of performance test to run')
        string(name: 'USERS', defaultValue: '100', description: 'Number of concurrent users')
        string(name: 'SPAWN_RATE', defaultValue: '10', description: 'Users spawned per second')
        string(name: 'RUN_TIME', defaultValue: '5m', description: 'Test duration (e.g., 5m, 10m, 1h)')
        booleanParam(name: 'HEADLESS', defaultValue: true, description: 'Run in headless mode (no web UI)')
    }

    environment {
        K8S_NAMESPACE = "${params.ENVIRONMENT}"
        REPORT_NAME = "performance-report-${params.ENVIRONMENT}-${BUILD_NUMBER}"
    }

    options {
        buildDiscarder(logRotator(numToKeepStr: '20'))
        timeout(time: 2, unit: 'HOURS')
        timestamps()
    }

    stages {
        stage('Initialize') {
            steps {
                script {
                    echo """
========================================
  Performance Tests
========================================
Environment: ${params.ENVIRONMENT}
Test Type: ${params.TEST_TYPE}
Users: ${params.USERS}
Spawn Rate: ${params.SPAWN_RATE}
Duration: ${params.RUN_TIME}
Headless: ${params.HEADLESS}
========================================
"""
                }
            }
        }

        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Setup Port-Forward') {
            steps {
                script {
                    echo "ðŸ“¡ Setting up port-forward to API Gateway..."

                    sh """
                        set -e

                        echo "========================================="
                        echo "Setting up Port-Forward to API Gateway"
                        echo "========================================="

                        # Kill any existing port-forwards on port 18080
                        echo "Cleaning up any existing port-forwards..."
                        pkill -f 'kubectl.*port-forward.*18080' || true
                        lsof -ti:18080 | xargs -r kill -9 || true
                        sleep 2

                        # Start kubectl port-forward in background
                        echo "Starting port-forward on port 18080..."
                        kubectl --insecure-skip-tls-verify port-forward -n ${K8S_NAMESPACE} svc/api-gateway 18080:80 > /tmp/port-forward-perf.log 2>&1 &
                        PORT_FORWARD_PID=\$!
                        echo "Port-forward started with PID: \$PORT_FORWARD_PID"

                        # Save PID to file for cleanup
                        echo \$PORT_FORWARD_PID > /tmp/port-forward-perf.pid

                        # Wait for port-forward to be ready
                        echo "Waiting for port-forward to be ready..."
                        sleep 5

                        # Verify port-forward is still running
                        if ! ps -p \$PORT_FORWARD_PID > /dev/null; then
                            echo "âŒ Port-forward failed to start"
                            cat /tmp/port-forward-perf.log
                            exit 1
                        fi

                        echo "âœ“ Port-forward ready (PID: \$PORT_FORWARD_PID)"
                    """

                    env.API_GATEWAY_URL = "http://172.17.0.1:18080"
                    echo "âœ“ API Gateway URL: ${env.API_GATEWAY_URL}"
                }
            }
        }

        stage('Verify Services Health') {
            steps {
                script {
                    echo "ðŸ¥ Checking services health..."
                    sh """
                        set -e

                        echo "Testing connectivity to ${env.API_GATEWAY_URL}"
                        echo ""

                        # Test connectivity with retries
                        for i in 1 2 3 4 5; do
                            HTTP_CODE=\$(curl -s -o /dev/null -w "%{http_code}" ${env.API_GATEWAY_URL}/app/api/products --max-time 5 || echo "000")

                            if [ "\$HTTP_CODE" != "000" ]; then
                                echo "âœ“ API Gateway is reachable (HTTP \$HTTP_CODE)"
                                break
                            fi

                            if [ \$i -lt 5 ]; then
                                echo "Attempt \$i/5: Got HTTP \$HTTP_CODE, retrying in 5 seconds..."
                                sleep 5
                            else
                                echo "âŒ ERROR: Cannot reach API Gateway at ${env.API_GATEWAY_URL} after 5 attempts"
                                echo ""
                                echo "Port-forward logs:"
                                cat /tmp/port-forward-perf.log || true
                                echo ""
                                echo "API Gateway pods:"
                                kubectl --insecure-skip-tls-verify get pods -n ${K8S_NAMESPACE} -l app=api-gateway
                                exit 1
                            fi
                        done

                        echo ""
                        echo "Kubernetes Services Status:"
                        kubectl --insecure-skip-tls-verify get pods -n ${K8S_NAMESPACE} | grep -E "(api-gateway|product-service|order-service|user-service)" || true
                    """
                }
            }
        }

        stage('Install Dependencies') {
            steps {
                script {
                    echo "ðŸ“¦ Installing Locust and dependencies..."
                    sh """
                        cd tests/performance

                        # Clean up and recreate virtual environment
                        echo "Creating fresh Python virtual environment..."
                        rm -rf venv
                        python3 -m venv venv

                        # Activate virtual environment and install dependencies
                        . venv/bin/activate

                        echo "Upgrading pip..."
                        pip install --upgrade pip

                        echo "Installing requirements..."
                        pip install -r requirements.txt

                        echo "Verifying Locust installation..."
                        locust --version

                        deactivate
                    """
                }
            }
        }

        stage('Run Performance Tests') {
            steps {
                script {
                    echo """
========================================
ðŸš€ Starting Performance Tests
========================================
Target: ${env.API_GATEWAY_URL}
Test Type: ${params.TEST_TYPE}
Users: ${params.USERS}
Spawn Rate: ${params.SPAWN_RATE}
Duration: ${params.RUN_TIME}
========================================
"""

                    def headlessFlag = params.HEADLESS ? '--headless' : ''
                    def reportFlags = params.HEADLESS ? "--html ${env.REPORT_NAME}.html --csv ${env.REPORT_NAME}" : ''

                    try {
                        sh """
                            cd tests/performance

                            # Activate virtual environment
                            . venv/bin/activate

                            locust -f locustfile.py ${params.TEST_TYPE} \
                                --host=${env.API_GATEWAY_URL} \
                                --users ${params.USERS} \
                                --spawn-rate ${params.SPAWN_RATE} \
                                --run-time ${params.RUN_TIME} \
                                ${headlessFlag} \
                                ${reportFlags}

                            deactivate
                        """
                        
                        echo """
========================================
âœ“ Performance Tests Completed
========================================
"""
                    } catch (Exception e) {
                        echo "âš ï¸ Performance tests encountered issues: ${e.message}"
                        echo "This may indicate performance problems or test failures"
                        
                        // Don't fail the pipeline, just mark as unstable
                        currentBuild.result = 'UNSTABLE'
                    }
                }
            }
        }

        stage('Analyze Results') {
            when {
                expression { params.HEADLESS }
            }
            steps {
                script {
                    echo "ðŸ“Š Analyzing performance test results..."
                    
                    sh """
                        cd tests/performance
                        
                        if [ -f "${env.REPORT_NAME}_stats.csv" ]; then
                            echo ""
                            echo "========================================="
                            echo "  Performance Summary"
                            echo "========================================="
                            head -20 ${env.REPORT_NAME}_stats.csv
                            echo ""
                            
                            # Check for high error rates
                            ERRORS=\$(awk -F, 'NR>1 {sum+=\$8} END {print sum}' ${env.REPORT_NAME}_stats.csv || echo "0")
                            TOTAL=\$(awk -F, 'NR>1 {sum+=\$2} END {print sum}' ${env.REPORT_NAME}_stats.csv || echo "1")
                            ERROR_RATE=\$(echo "scale=2; \$ERRORS / \$TOTAL * 100" | bc || echo "0")
                            
                            echo "Total Requests: \$TOTAL"
                            echo "Failed Requests: \$ERRORS"
                            echo "Error Rate: \$ERROR_RATE%"
                            
                            # Fail if error rate is too high (> 5%)
                            if (( \$(echo "\$ERROR_RATE > 5.0" | bc -l) )); then
                                echo "âŒ ERROR RATE TOO HIGH: \$ERROR_RATE%"
                                exit 1
                            fi
                        else
                            echo "âš ï¸ No stats file found"
                        fi
                    """
                }
            }
        }

        stage('Publish Results') {
            when {
                expression { params.HEADLESS && fileExists("tests/performance/${env.REPORT_NAME}.html") }
            }
            steps {
                script {
                    echo "ðŸ“¤ Publishing performance test reports..."
                    
                    publishHTML([
                        reportDir: 'tests/performance',
                        reportFiles: "${env.REPORT_NAME}.html",
                        reportName: "Performance Test Report - ${params.ENVIRONMENT}",
                        keepAll: true,
                        alwaysLinkToLastBuild: true
                    ])
                    
                    // Archive CSV results
                    archiveArtifacts artifacts: "tests/performance/${env.REPORT_NAME}*.csv", allowEmptyArchive: true
                    archiveArtifacts artifacts: "tests/performance/${env.REPORT_NAME}.html", allowEmptyArchive: true
                    
                    echo "âœ“ Reports published successfully"
                }
            }
        }

        stage('SonarQube Analysis - Performance Tests') {
            steps {
                script {
                    echo "ðŸ“Š Analyzing performance test code with SonarQube..."

                    try {
                        sh """
                            cd tests/performance

                            # Download SonarQube Scanner if not present
                            if [ ! -d "sonar-scanner" ]; then
                                echo "Downloading SonarQube Scanner..."
                                wget -q https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-5.0.1.3006-linux.zip
                                unzip -q sonar-scanner-cli-5.0.1.3006-linux.zip
                                mv sonar-scanner-5.0.1.3006-linux sonar-scanner
                                rm sonar-scanner-cli-5.0.1.3006-linux.zip
                            fi

                            # Run SonarQube analysis
                            echo "Running SonarQube analysis..."
                            ./sonar-scanner/bin/sonar-scanner \\
                                -Dsonar.host.url=http://172.17.0.1:9000 \\
                                -Dsonar.token=squ_1037e66e9bc493d2a288dbca5a9cb503f0637c93 \\
                                -Dsonar.projectKey=ecommerce-performance-tests \\
                                -Dsonar.projectName="E-Commerce Performance Tests" \\
                                -Dsonar.projectVersion=1.0 \\
                                -Dsonar.sources=. \\
                                -Dsonar.inclusions=**/*.py \\
                                -Dsonar.exclusions=**/venv/**,**/__pycache__/**,**/sonar-scanner/** \\
                                -Dsonar.language=py \\
                                -Dsonar.python.version=3.13 \\
                                -Dsonar.sourceEncoding=UTF-8

                            echo "âœ“ SonarQube analysis completed"
                            echo "View results at: http://172.17.0.1:9000/dashboard?id=ecommerce-performance-tests"
                        """
                    } catch (Exception e) {
                        echo "âš ï¸ SonarQube analysis failed: ${e.message}"
                        echo "This is non-blocking, continuing pipeline..."
                    }
                }
            }
        }

        stage('Summary') {
            steps {
                script {
                    echo """
========================================
  Performance Test Summary
========================================
âœ“ Environment: ${params.ENVIRONMENT}
âœ“ Test Type: ${params.TEST_TYPE}
âœ“ Users: ${params.USERS}
âœ“ Spawn Rate: ${params.SPAWN_RATE}
âœ“ Duration: ${params.RUN_TIME}
âœ“ Report: ${env.REPORT_NAME}.html

View Results:
- HTML Report: Check Published HTML Reports
- CSV Data: Check Build Artifacts
- Full Logs: See console output above
- SonarQube: http://172.17.0.1:9000/dashboard?id=ecommerce-performance-tests

Next Steps:
1. Review report for performance bottlenecks
2. Check error rates and response times
3. Compare with previous test runs
4. Review SonarQube for code quality issues
5. Scale services if needed
========================================
"""
                }
            }
        }
    }

    post {
        success {
            script {
                echo "âœ… Performance tests completed successfully"
            }
        }
        unstable {
            script {
                echo "âš ï¸ Performance tests completed with warnings"
                echo "Check the report for high error rates or slow response times"
            }
        }
        failure {
            script {
                echo "âŒ Performance tests failed"
                echo "Collecting diagnostic information..."
                
                sh """
                    kubectl --insecure-skip-tls-verify get pods -n ${K8S_NAMESPACE} || true
                    kubectl --insecure-skip-tls-verify top nodes || true
                    kubectl --insecure-skip-tls-verify top pods -n ${K8S_NAMESPACE} || true
                """
            }
        }
        always {
            script {
                echo "ðŸ§¹ Cleaning up resources..."

                // Cleanup port-forward
                sh '''
                    if [ -f /tmp/port-forward-perf.pid ]; then
                        PID=$(cat /tmp/port-forward-perf.pid)
                        echo "Stopping port-forward (PID: $PID)..."
                        kill $PID 2>/dev/null || true
                        rm -f /tmp/port-forward-perf.pid /tmp/port-forward-perf.log
                    fi

                    # Also kill any lingering port-forwards
                    pkill -f 'kubectl.*port-forward.*18080' || true
                '''
            }

            cleanWs(deleteDirs: true, patterns: [
                [pattern: 'tests/performance/*.log', type: 'INCLUDE']
            ])
        }
    }
}
